

---
### 标题: Diffusion Models Are Real-Time Game Engines

### 链接: https://gamengen.github.io

#### 正文: [



              We present GameNGen, the first game engine powered entirely by a neural model
              that enables real-time interaction with a complex environment over long trajectories at high quality.
              GameNGen can interactively simulate the classic game DOOM at over 20 frames per second on a single TPU.
              Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression.
              Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation.
              GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and
              (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions.
              Conditioning augmentations enable stable auto-regressive generation over long trajectories.
            



Data Collection via Agent Play:
                Since we cannot collect human gameplay at scale, as a first stage we train an automatic RL-agent to play
                the game,
                persisting it&#39;s training episodes of actions and observations, which become the training data for our
                generative model.
              

Training the Generative Diffusion Model: We re-purpose a small diffusion model, Stable Diffusion v1.4,
                and condition it on a sequence of previous actions and observations (frames). To mitigate
                auto-regressive drift
                during inference, we corrupt context frames by adding Gaussian noise to encoded frames during training.
                This allows the network to correct information sampled in previous frames, and we found it to be
                critical
                for preserving visual stability over long time periods.
              

Latent Decoder Fine-Tuning: The pre-trained auto-encoder of Stable Diffusion v1.4, which compresses
                8x8
                pixel patches into 4 latent channels, results in meaningful artifacts when predicting game frames, which
                affect
                small details and particularly the bottom bar HUD. To leverage the pre-trained knowledge while improving
                image quality, we train just the decoder of the latent auto-encoder using an MSE loss computed
                against the target frame pixels.
              


              This page was built using the Academic Project Page Template which was adopted from the Nerfies project page.
            
]


---
### 标题: Microsoft donates the Mono Project to the Wine team

### 链接: https://www.mono-project.com/

#### 正文: [


The Mono Project (mono/mono) (‘original mono’) has been an important part of the .NET ecosystem since it was launched in 2001. Microsoft became the steward of the Mono Project when it acquired Xamarin in 2016.

The last major release of the Mono Project was in July 2019, with minor patch releases since that time. The last patch release was February 2024.

We are happy to announce that the WineHQ organization will be taking over as the stewards of the Mono Project upstream at wine-mono / Mono · GitLab (winehq.org).  Source code in existing mono/mono and other repos will remain available, although repos may be archived. Binaries will remain available for up to four years.

Microsoft maintains a modern fork of Mono runtime in the dotnet/runtime repo and has been progressively moving workloads to that fork. That work is now complete, and we recommend that active Mono users and maintainers of Mono-based app frameworks migrate to .NET which includes work from this fork.

We want to recognize that the Mono Project was the first .NET implementation on Android, iOS, Linux, and other operating systems. The Mono Project was a trailblazer for the .NET platform across many operating systems. It helped make cross-platform .NET a reality and enabled .NET in many new places and we appreciate the work of those who came before us.

Thank you to all the Mono developers!


      Sponsored by Microsoft, Mono is
      an open source implementation of Microsoft&#39;s .NET Framework as part of
      the .NET Foundation and
      based on the ECMA
      standards
      for C# and
      the Common Language
    Runtime.
      A growing family of solutions and an active and enthusiastic contributing community is helping position Mono to become the leading choice for development of cross platform applications.
    

The latest Mono release is waiting for you!

Download


          We cover everything you need to know, from configuring Mono to how the internals are implemented.
          
Our documentation is open source too, so you can help us improve it.


Learn more


          As an open source project, we love getting contributions from the community.
          
File a bug report, add new code or chat with the developers.


Contribute to Mono
]


---
### 标题: Cosmic Alpha Released

### 链接: https://blog.system76.com/post/cosmic-alpha-released-heres-what-people-are-saying/

#### 正文: [
 
        Monday, August 26, 2024
       
        COSMIC Alpha Released! Here’s what people are saying.
           It’s happening! This is not a drill! The alpha version of COSMIC, our new desktop environment for Pop!_OS and other Linux distros, has been released. COSMIC adds new features, customization, performance, stability, and security. Its “alpha” state adds bugs. Bug reports are welcome, as are screenshots of your custom themes and panels.To make sure people are as excited about COSMIC as we are, it was essential to give it the best impression we could. Even for an alpha, most features required for daily use needed to feel polished. However, you’re bound to come across some breaking bugs before the official release, so we advise you hold off on using COSMIC for production use for now.Here’s how to try it.Try COSMIC on Pop!_OSDownload Intel/AMD ISOsha256sum 894bc15abcad05839b226655121a113ad16cfbc4ada98e93e3ffb74a853fdcd4Download NVIDIA ISOsha256sum 3d636b705c1049395d50bbb5acd7c709fd871de78e4d95d297bcbdab7cae4e05Try COSMIC on your favorite distroInstructions are available for installing COSMIC on Fedora, NixOS, Arch, OpenSUSE, and many more here.First ImpressionsWe gave folks around the Linuxsphere some time to play with the Alpha COSMIC ISO. Feedback so far has overall been fairly glowing! Check out what they’re saying about their time with COSMIC:“I tried this COSMIC desktop on a very low-end system, and it was still impressively, impressively fast.” — Linux Unplugged podcast&#34;The foundation seems very solid, and I wouldn&#39;t be surprised if, in a year or two, this thing was considered THE default desktop that you would recommend to anyone.&#34; — The Linux Experiment“There’s such cool power-user potential here.” — Michael Tunnell“What I can say about Cosmic, even at this early alpha stage, is that it&#39;s relatively snappy and cohesive compared to other systems I&#39;ve used.” — Ars Technica“I can imagine this becoming really popular.” — GamingOnLinux“I&#39;ve been playing around with this Pop!_OS 24.04 alpha…and it&#39;s been working out quite well.” — Phoronix“Despite the alpha tag, everything runs well for daily driver usage.” — OMG! Ubuntu“It’s pretty impressive how far they’ve come in two years working on this.” — Gardiner Bryant“They have really focused on refining the user experience.” — SavvyNik“I could tile windows effortlessly, with keyboard shortcuts letting me navigate between windows and workspaces without touching the mouse.” — It’s FOSS“It has options for either vertical or horizontal workspaces, an integrated tiling system, and customizable panels for the dock and top bar.” — How-To Geek“It’s a very good desktop. I love the functionality, it looks great, but of course there’s going to be some bugs.” — Learn Linux TV“Now I’m more excited than ever.” — Brodie Robertson, after some COSMIC Q&amp;A. He also had some helpful feedback for us!“The new COSMIC desktop environment in Pop!_OS 24.04 LTS provides a solid foundation for rapid advancement long-term, a holistic design system that’s modern, fast, and responsive for a consistent software ecosystem.” — 9to5Linux“COSMIC is designed to be a modern, customizable, and performant desktop environment.” — OSTechnix“After the first few seconds, you already know — you’re home.” — Linuxiac“COSMIC is a significant update to the previous Pop!_OS interface, offering a more modern and user-friendly experience.” — Nix SanctuaryThe design systemThe design system determines what makes COSMIC look and feel like COSMIC. This includes typography, colors and theming system, spacing, corner radii, and sizing across COSMIC’s interface, as well as what widgets are used (or yet to be implemented for use) in COSMIC apps. Making an official design system available, albeit one still a work in progress was essential for this first release, as it also serves as a guide for anyone building an app or applet — so that it feels integrated with the rest of the desktop environment. Now app developers can provide the best possible experience early on.App and applet templateWe’ve finalized an official app template for best practices on what to include in apps developed for COSMIC. Items like support for the Launcher, what types of icons to provide, and descriptions for app stores are listed in the app template.Likewise, check out the COSMIC applet template for building applets. Create a new repository from this template and clone it to your local machine to get started on making your applet.Thanks to the awesome edfloreshz for the assist!Pop!_OS 24.04 LTSThe COSMIC alpha on Pop!_OS is also an alpha for the latest Pop!_OS 24.04 LTS. When COSMIC Epoch 1 is officially released, this will be made available as an upgrade through the normal upgrade path in the OS. An upgrade path will also be available in the second alpha for testers.Date &amp; Time SettingsSettings for adjusting date, time, and time zone have been implemented! The time is set automatically based on your selected time zone.Screen CaptureShare your experience with COSMIC! We’ve built functionality into xdg-desktop portal screen capture that allows you to select an output, as well as select a certain window to record.Touchpad defaultsChanges were made to the compositor to set tap-to-click, as well as disabling the touchpad while typing, as defaults.Building the ISOWrapping up the COSMIC alpha, we did a cursory test of the ISO file to get the installation working. As part of that process, screen-sharing in video conferencing apps is now functional.Contributions from YOUThanks to our contributors for helping COSMIC reach its full potential!Koranir added a feature for using Super + Right-click + drag to resize windows.leb-kuchen added a feature for double-clicking title bars to maximize a window for server-side decorations.Additionally, they fixed a bug with minimize and maximize buttons where certain applications weren’t respecting COSMIC users’ preferences on which buttons would be shown.Become a COSMIC AmbassadorCOSMIC Ambassadors are amazing people who contribute to COSMIC or promote it on social media. They also get free swag! You, too, can become an ambassador by filling out this short form.A lot has been developed over these past two years, but there’s still quite a bit left to be done. Here’s some areas where you can assist:System &amp; Accounts &gt; UsersRegion &amp; LanguageDesktop &gt; Window ManagementNetwork &amp; WirelessBluetoothPower and BatteryOS Update and RecoveryDefault Applications  
      The COSMIC logo mark and surrounding pattern. COSMIC is made for any device with a display.
    

It’s happening! This is not a drill! The alpha version of COSMIC, our new desktop environment for Pop!_OS and other Linux distros, has been released. COSMIC adds new features, customization, performance, stability, and security. Its “alpha” state adds bugs. Bug reports are welcome, as are screenshots of your custom themes and panels.



To make sure people are as excited about COSMIC as we are, it was essential to give it the best impression we could. Even for an alpha, most features required for daily use needed to feel polished. However, you’re bound to come across some breaking bugs before the official release, so we advise you hold off on using COSMIC for production use for now.

Here’s how to try it.





Try COSMIC on Pop!_OS

Download Intel/AMD ISOsha256sum 894bc15abcad05839b226655121a113ad16cfbc4ada98e93e3ffb74a853fdcd4

Download NVIDIA ISOsha256sum 3d636b705c1049395d50bbb5acd7c709fd871de78e4d95d297bcbdab7cae4e05





Try COSMIC on your favorite distro

Instructions are available for installing COSMIC on Fedora, NixOS, Arch, OpenSUSE, and many more here.





First Impressions

We gave folks around the Linuxsphere some time to play with the Alpha COSMIC ISO. Feedback so far has overall been fairly glowing! Check out what they’re saying about their time with COSMIC:

“I tried this COSMIC desktop on a very low-end system, and it was still impressively, impressively fast.” — Linux Unplugged podcast





&#34;The foundation seems very solid, and I wouldn&#39;t be surprised if, in a year or two, this thing was considered THE default desktop that you would recommend to anyone.&#34; — The Linux Experiment



“There’s such cool power-user potential here.” — Michael Tunnell





“What I can say about Cosmic, even at this early alpha stage, is that it&#39;s relatively snappy and cohesive compared to other systems I&#39;ve used.” — Ars Technica





“I can imagine this becoming really popular.” — GamingOnLinux



“I&#39;ve been playing around with this Pop!_OS 24.04 alpha…and it&#39;s been working out quite well.” — Phoronix





“Despite the alpha tag, everything runs well for daily driver usage.” — OMG! Ubuntu





“It’s pretty impressive how far they’ve come in two years working on this.” — Gardiner Bryant



“They have really focused on refining the user experience.” — SavvyNik





“I could tile windows effortlessly, with keyboard shortcuts letting me navigate between windows and workspaces without touching the mouse.” — It’s FOSS



“It has options for either vertical or horizontal workspaces, an integrated tiling system, and customizable panels for the dock and top bar.” — How-To Geek





“It’s a very good desktop. I love the functionality, it looks great, but of course there’s going to be some bugs.” — Learn Linux TV



“Now I’m more excited than ever.” — Brodie Robertson, after some COSMIC Q&amp;A. He also had some helpful feedback for us!



“The new COSMIC desktop environment in Pop!_OS 24.04 LTS provides a solid foundation for rapid advancement long-term, a holistic design system that’s modern, fast, and responsive for a consistent software ecosystem.” — 9to5Linux



“COSMIC is designed to be a modern, customizable, and performant desktop environment.” — OSTechnix





“After the first few seconds, you already know — you’re home.” — Linuxiac





“COSMIC is a significant update to the previous Pop!_OS interface, offering a more modern and user-friendly experience.” — Nix Sanctuary



The design system

The design system determines what makes COSMIC look and feel like COSMIC. This includes typography, colors and theming system, spacing, corner radii, and sizing across COSMIC’s interface, as well as what widgets are used (or yet to be implemented for use) in COSMIC apps. Making an official design system available, albeit one still a work in progress was essential for this first release, as it also serves as a guide for anyone building an app or applet — so that it feels integrated with the rest of the desktop environment. Now app developers can provide the best possible experience early on.





App and applet template

We’ve finalized an official app template for best practices on what to include in apps developed for COSMIC. Items like support for the Launcher, what types of icons to provide, and descriptions for app stores are listed in the app template.



Likewise, check out the COSMIC applet template for building applets. Create a new repository from this template and clone it to your local machine to get started on making your applet.



Thanks to the awesome edfloreshz for the assist!





Pop!_OS 24.04 LTS

The COSMIC alpha on Pop!_OS is also an alpha for the latest Pop!_OS 24.04 LTS. When COSMIC Epoch 1 is officially released, this will be made available as an upgrade through the normal upgrade path in the OS. An upgrade path will also be available in the second alpha for testers.





Date &amp; Time Settings

Settings for adjusting date, time, and time zone have been implemented! The time is set automatically based on your selected time zone.



Screen Capture

Share your experience with COSMIC! We’ve built functionality into xdg-desktop portal screen capture that allows you to select an output, as well as select a certain window to record.





Touchpad defaults

Changes were made to the compositor to set tap-to-click, as well as disabling the touchpad while typing, as defaults.



Building the ISO

Wrapping up the COSMIC alpha, we did a cursory test of the ISO file to get the installation working. As part of that process, screen-sharing in video conferencing apps is now functional.





Contributions from YOU

Thanks to our contributors for helping COSMIC reach its full potential!



Become a COSMIC Ambassador

COSMIC Ambassadors are amazing people who contribute to COSMIC or promote it on social media. They also get free swag! You, too, can become an ambassador by filling out this short form.



A lot has been developed over these past two years, but there’s still quite a bit left to be done. Here’s some areas where you can assist:
]


---
### 标题: Boxxy puts bad Linux applications in a box with only their files

### 链接: https://github.com/queer/boxxy

#### 正文: [
boxxy
boxxy (case-sensitive) is a tool for boxing up misbehaving Linux applications
and forcing them to put their files and directories in the right place,
without symlinks!
boxxy is a part of the amyware discord server.
If you like what I make, consider supporting me on Patreon:

Linux-only! boxxy uses Linux namespaces for its functionality.
For example, consider tmux. It wants to put its config in ~/.tmux.conf. With
boxxy, you can put its config in ~/.config/tmux/tmux.conf instead:
# ~/.config/boxxy/boxxy.yaml
rules:
- name: &#34;redirect tmux config from ~/.tmux.conf to ~/.config/tmux/tmux.conf&#34;
  target: &#34;~/.tmux.conf&#34;
  rewrite: &#34;~/.config/tmux/tmux.conf&#34;
  mode: &#34;file&#34;

motivation
I recently had to use the AWS CLI. It wants to save data in ~/.aws, but I
don&#39;t want it to just clutter up my $HOME however it wants. boxxy lets me
force it to puts its data somewhere nice and proper.
features

box any program and force it to put its files/directories where you want it to
context-dependent boxing, ie different rules apply in different directories
depending on your configuration
minimal overhead
opt-in immutable fs outside of rule rewrites, ie only the files/directories
you specify in rules are writable
0.5.0: boxxy can scan your homedir to automatically suggest rules for
you! 
0.6.0: boxxy can use project-local boxxy.yaml files, and can load
.env files for you! 
0.6.1: boxxy rules can inject env vars: 
0.7.2: boxxy can fork the boxxed process into the background with the
--daemon flag.
0.8.0: boxxy can pass rules at the command line with --rule, and disable
loading config files with --no-config.
0.8.2: Explain how to run AppImages properly: 

potential drawbacks

new project, 0.x.y, comes with all those warnings
cannot use sudo inside the container (see #6)
primarily tested for my use-cases

example usage
git:(mistress) | ▶  cat ~/.config/boxxy/boxxy.yaml
rules:
- name: &#34;Store AWS CLI config in ~/.config/aws&#34;
  target: &#34;~/.aws&#34;
  rewrite: &#34;~/.config/aws&#34;

git:(mistress) | ▶  boxxy aws configure
 INFO  boxxy &gt; loaded 1 rules
 INFO  boxxy::enclosure &gt; applying rule &#39;Store AWS CLI config in ~/.config/aws&#39;
 INFO  boxxy::enclosure &gt; redirect: ~/.aws -&gt; ~/.config/aws
 INFO  boxxy::enclosure &gt; boxed &#34;aws&#34; ♥
AWS Access Key ID [****************d]: a
AWS Secret Access Key [****************c]: b
Default region name [b]: c
Default output format [a]: d
git:(mistress) | ▶  ls ~/.aws
git:(mistress) | ▶  ls ~/.config/aws
config  credentials
git:(mistress) | ▶  cat ~/.config/aws/config
[default]
region = c
output = d
git:(mistress) | ▶
suggested usage

alias aws=&#34;boxxy aws&#34; (repeat for other tools)
use contexts to keep project configs separate on disk
dotfiles!
stop using symlinks!!!
no more dev config files when writing code

configuration
The boxxy configuration file lives in ~/.config/boxxy/boxxy.yaml. If none
exists, an empty one will be created for you.
rules:
# The name of the rule. User-friendly name for your reference
- name: &#34;redirect aws-cli from ~/.aws to ~/.config/aws&#34;
  # The target of the rule, ie the file/directory that will be shadowed by the
  # rewrite.
  target: &#34;~/.aws&#34;
  # The rewrite of the rule, ie the file/directory that will be used instead of
  # the target.
  rewrite: &#34;~/.config/aws&#34;
- name: &#34;use different k8s configs when in ~/Projects/my-cool-startup&#34;
  target: &#34;~/.kube/config&#34;
  rewrite: &#34;~/Projects/my-cool-startup/.kube/config&#34;
  # The context for the rule. Any paths listed in the context are paths where
  # this rule will apply. If no context is specified, the rule applies
  # globally.
  context:
  - &#34;~/Projects/my-cool-startup&#34;
  # The mode of this rule, either `directory` or `file`. `directory` is the
  # default. Must be specified for the correct behaviour when the target is a
  # file. Required because the target file/directory may not exist yet.
  mode: &#34;file&#34;
  # The list of commands that this rule applies to. If no commands are
  # specified, the rule applies to all programs run with boxxy.
  only:
  - &#34;kubectl&#34;
syntax
rules:
- name: &#34;any valid string&#34; # required
  target: &#34;path&#34; # required
  rewrite: &#34;path&#34; # required
  context: # optional
  - &#34;path&#34;
  - &#34;path&#34;
  mode: &#34;directory | file&#34; # optional
  only: # optional
  - &#34;binary name&#34;
  - &#34;binary name&#34;
  env: # optional
    KEY: &#34;value&#34;
developing

set up pre-commit: pre-commit install
make sure it builds: cargo build
do the thing!
test with the command of your choice, ex. cargo run -- ls -lah ~/.config

how does it work?

create temporary directory in /tmp
set up new user/mount namespace
bind-mount / to tmp directory
bind-mount rule mounts rw so that target programs can use them
remount / ro
run!

credits

fixtures/helloworld-appimage-x86_84.AppImage: https://github.com/ClonedRepos/hello-world-appimage



We read every piece of feedback, and take your input very seriously.


            To see all available qualifiers, see our documentation.
          


        boxxy puts bad Linux applications in a box with only their files.
      

boxxy (case-sensitive) is a tool for boxing up misbehaving Linux applications
and forcing them to put their files and directories in the right place,
without symlinks!

boxxy is a part of the amyware discord server.

If you like what I make, consider supporting me on Patreon:



Linux-only! boxxy uses Linux namespaces for its functionality.

For example, consider tmux. It wants to put its config in ~/.tmux.conf. With
boxxy, you can put its config in ~/.config/tmux/tmux.conf instead:



I recently had to use the AWS CLI. It wants to save data in ~/.aws, but I
don&#39;t want it to just clutter up my $HOME however it wants. boxxy lets me
force it to puts its data somewhere nice and proper.

The boxxy configuration file lives in ~/.config/boxxy/boxxy.yaml. If none
exists, an empty one will be created for you.


        boxxy puts bad Linux applications in a box with only their files.
      
]


---
### 标题: Final Two Communications from MH370 Support Controlled Eastward Descent Scenario

### 链接: https://www.researchgate.net/publication/355242503_Final_Two_Communications_from_MH370_Supports_Controlled_Eastward_Descent_Scenario

#### 正文: [

]


---
### 标题: The Fall of StackOverflow: A Data-Driven Analysis

### 链接: https://pdftranslate.ai/blog/stackoverflow-fall

#### 正文: [


Did ChatGPT kill StackOverflow, or was it already dying?

In recent years, the rise of advanced AI tools like GitHub Copilot and ChatGPT has sparked discussions about the future of StackOverflow. Many articles and posts suggest that StackOverflow is in decline, attributing its struggles to these new technologies. While these claims are certainly understandble and may may be intuitive, it&#39;s crucial to examine the data behind StackOverflow&#39;s performance to understand the broader trends.

Let&#39;s look at some data from the last 16 years.

Question Count

The Questions Count chart shows a steady increase from 2008 to 2013, followed by stagnation until 2018, when a gradual decline began. This decline accelerated sharply after ChatGPT&#39;s release in November 2022, but the decrease had started earlier, indicating pre-existing issues beyond the impact of the new AI. This is the raw data used for this observation.

Answer Count

The Answers Count chart reflects similar trends to the Questions Count chart, with a rise from 2008 to 2013, stagnation, and a decline starting in 2018. Both datasets saw a sharper drop after ChatGPT&#39;s release, and yet the decrease was already there before, pointing to issues beyond the AI&#39;s introduction. This is the raw data used for this observation.

View Count

The Views Count chart reveals an even clearer trend of constant decline starting in 2013, highlighting significant issues with the model StackOverflow operated under. This is the raw data used for this observation.

Let&#39;s list a few reasons mentioned by many on both social media and platforms like Reddit:
                    1. Questions became outdated, turning into a competition of which answer was better and more upvoted, which sometimes didn&#39;t yield useful results.
                    2. Heavy moderation discouraged people from asking questions.
                    3. The platform failed to encourage user engagement, leading to a decline in posts and answers after its peak popularity in 2012.
                    4. The UI remained outdated for a decade, while newer, more innovative approaches like chat-based interfaces emerged as alternatives.
                    Summary
StackOverflow&#39;s decline was a result of issues within its model and execution, which predated ChatGPT&#39;s introduction. While ChatGPT may have accelerated this decline, it was not the sole cause. There are likely many factors contributing to StackOverflow&#39;s struggles, but AI alone cannot account for the entirety of its challenges.


Summary
StackOverflow&#39;s decline was a result of issues within its model and execution, which predated ChatGPT&#39;s introduction. While ChatGPT may have accelerated this decline, it was not the sole cause. There are likely many factors contributing to StackOverflow&#39;s struggles, but AI alone cannot account for the entirety of its challenges.

StackOverflow&#39;s decline was a result of issues within its model and execution, which predated ChatGPT&#39;s introduction. While ChatGPT may have accelerated this decline, it was not the sole cause. There are likely many factors contributing to StackOverflow&#39;s struggles, but AI alone cannot account for the entirety of its challenges.
]


---
### 标题: Faster CRDTs (2021)

### 链接: https://josephg.com/blog/crdts-go-brrr/

#### 正文: [


July 31 2021

A few years ago I was really bothered by an academic paper.

Some researchers in France put together a comparison showing lots of ways you could implement realtime collaborative editing (like Google Docs). They implemented lots of algorithms - CRDTs and OT algorithms and stuff. And they benchmarked them all to see how they perform. (Cool!!) Some algorithms worked reasonably well. But others took upwards of 3 seconds to process simple paste operations from their editing sessions. Yikes!

Which algorithm was that? Well, this is awkward but .. it was mine. I mean, I didn&#39;t invent it - but it was the algorithm I was using for ShareJS. The algorithm we used for Google Wave. The algorithm which - hang on - I knew for a fact didn&#39;t take 3 seconds to process large paste events. Whats going on here?

I took a closer look at the paper. In their implementation when a user pasted a big chunk of text (like 1000 characters), instead of creating 1 operation with 1000 characters, their code split the insert into 1000 individual operations. And each of those operations needed to be processed separately. Do&#39;h - of course it&#39;ll be slow if you do that! This isn&#39;t a problem with the operational transformation algorithm. This is just a problem with their particular implementation.

The infuriating part was that several people sent me links to the paper and (pointedly) asked me what I think about it. Written up as a Published Science Paper, these speed comparisons seemed like a Fact About The Universe. And not what they really were - implementation details of some java code, written by a probably overstretched grad student. One of a whole bunch of implementations that they needed to code up.

&#34;Nooo! The peer reviewed science isn&#39;t right everybody! Please believe me!&#34;. But I didn&#39;t have a published paper justifying my claims. I had working code but it felt like none of the smart computer science people cared about that. Who was I? I was nobody.

Even talking about this stuff we have a language problem. We describe each system as an &#34;algorithm&#34;. Jupiter is an Algorithm. RGA is an Algorithm. But really there are two very separate aspects:

If some academic&#39;s code runs slowly, what does that actually teach us? Maybe it&#39;s like tests. A passing test suite suggests, but can never prove that there are no bugs. Likewise a slow implementation suggests, but can never prove that every implementation of the system will be slow. If you wait long enough, somebody will find more bugs. And, maybe, someone out there can design a faster implementation.

Years ago I translated my old text OT code into C, Javascript, Go, Rust and Swift. Each implementation has the same behaviour, and the same algorithm. But the performance is not even close. In javascript my transform function ran about 100 000 times per second. Not bad! But the same function in C does 20M iterations per second. That&#39;s 200x faster. Wow!

Were the academics testing a slow version or the fast version of this code? Maybe, without noticing, they had fast versions of some algorithms and slow versions of others. It&#39;s impossible to tell from the paper!

So as you may know, I&#39;ve been getting interested in CRDTs lately. For the uninitiated, CRDTs (Conflict-Free Replicated Data types) are fancy programming tools which let multiple users edit the same data at the same time. They let you work locally with no lag. (You don&#39;t even have to be online). And when you do sync up with other users &amp; devices, everything just magically syncs up and becomes eventually consistent. The best part of CRDTs is that they can do all that without even needing a centralized computer in the cloud to monitor and control everything.

I want Google Docs without google. I want my apps to seamlessly share data between all my devices, without me needing to rely on some flakey startup&#39;s servers to still be around in another decade. I think they&#39;re the future of collaborative editing. And maybe the future of all software - but I&#39;m not ready to talk about that yet.

But most CRDTs you read about in academic papers are crazy slow. A decade ago I decided to stop reading academic papers and dismissed them. I assumed CRDTs had some inherent problem. A GUID for every character? Nought but madness comes from those strange lands! But - and this is awkward to admit - I think I&#39;ve been making the same mistake as those researchers. I was reading papers which described the behaviour of different systems. And I assumed that meant we knew how the best way to implement those systems. And wow, I was super wrong.

How wrong? Well. Running this editing trace, Automerge (a popular CRDT, written by a popular researcher) takes nearly 5 minutes to run. I have a new implementation that can process the same editing trace in 56 milliseconds. Thats 0.056 seconds, which is over 5000x faster. It&#39;s the largest speed up I&#39;ve ever gotten from optimization work - and I&#39;m utterly delighted by it.

Lets talk about why automerge is currently slow, and I&#39;ll take you through all the steps toward making it super fast.

Wait, no. First we need to start with:

Automerge is a library to help you do collaborative editing. It&#39;s written by Martin Kleppmann, who&#39;s a little bit famous from his book and excellent talks. Automerge is based on an algorithm called RGA, which you can read about in an academic paper if you&#39;re into that sort of thing.

Martin explains automerge far better than I will in this talk from 2020:

Automerge (and Yjs and other CRDTs) think of a shared document as a list of characters. Each character in the document gets a unique ID, and whenever you insert into the document, you name what you&#39;re inserting after.

Imagine I type &#34;abc&#34; into an empty document. Automerge creates 3 items:

We can draw this as a tree!



Lets say Mike inserts an &#39;X&#39; between a and b, so we get &#34;aXbc&#34;. Then we have:



Note the &#39;X&#39; and &#39;b&#39; both share the same parent. This will happen when users type concurrently in the same location in the document. But how do we figure out which character goes first? We could just sort using their agent IDs or something. But argh, if we do that the document could end up as abcX, even though Mike inserted X before the b. That would be really confusing.

Automerge (RGA) solves this with a neat hack. It adds an extra integer to each item called a sequence number. Whenever you insert something, you set the new item&#39;s sequence number to be 1 bigger than the biggest sequence number you&#39;ve ever seen:

This is the algorithmic version of &#34;Wow I saw a sequence number, and it was this big!&#34; &#34;Yeah? Mine is even bigger!&#34;

The rule is that children are sorted first based on their sequence numbers (bigger sequence number first). If the sequence numbers match, the changes must be concurrent. In that case we can sort them arbitrarily based on their agent IDs. (We do it this way so all peers end up with the same resulting document.)

Yjs - which we&#39;ll see more of later - implements a CRDT called YATA. YATA is identical to RGA, except that it solves this problem with a slightly different hack. But the difference isn&#39;t really important here.

Automerge (RGA)&#39;s behaviour is defined by this algorithm:

So how should you implement automerge? The automerge library does it in the obvious way, which is to store all the data as a tree. (At least I think so - after typing &#34;abc&#34; this is automerge&#39;s internal state. Uh, uhm, I have no idea whats going on here. And what are all those Uint8Arrays doing all over the place? Whatever.) The automerge library works by building a tree of items.

For a simple benchmark, I&#39;m going to test automerge using an editing trace Martin himself made. This is a character by character recording of Martin typing up an academic paper. There aren&#39;t any concurrent edits in this trace, but users almost never actually put their cursors at exactly the same place and type anyway, so I&#39;m not too worried about that. I&#39;m also only counting the time taken to apply this trace locally, which isn&#39;t ideal but it&#39;ll do. Kevin Jahns (Yjs&#39;s author) has a much more extensive benchmarking suite here if you&#39;re into that sort of thing. All the benchmarks here are done on my chonky ryzen 5800x workstation, with Nodejs v16.1 and rust 1.52 when that becomes appropriate. (Spoilers!)

The editing trace has 260 000 edits, and the final document size is about 100 000 characters.

As I said above, automerge takes a little under 5 minutes to process this trace. Thats just shy of 900 edits per second, which is probably fine. But by the time it&#39;s done, automerge is using 880 MB of RAM. Whoa! That&#39;s 10kb of ram per key press. At peak, automerge was using 2.6 GB of RAM!

To get a sense of how much overhead there is, I&#39;ll compare this to a baseline benchmark where we just splice all the edits directly into a javascript string. This throws away all the information we need to do collaborative editing, but it gives us a sense of how fast javascript is capable of going. It turns out javascript running on V8 is fast:

This is a chart showing the time taken to process each operation throughout the test, averaged in groups of 1000 operations. I think those spikes are V8&#39;s garbage collector trying to free up memory.



In the slowest spike near the end, a single edit took 1.8 seconds to process. Oof. In a real application, the whole app (or browser tab) would freeze up for a couple of seconds sometimes while you&#39;re in the middle of typing.

The chart is easier to read when we average everything out a bit and zoom the Y axis. We can see the average performance gets gradually (roughly linearly) worse over time.



Automerge is slow for a whole slew of reasons:

Automerge was just never written with performance in mind. Their team is working on a replacement rust implementation of the algorithm to run through wasm, but at the time of writing it hasn&#39;t landed yet. I got the master branch working, but they have some kinks to work out before it&#39;s ready. Switching to the automerge-rs backend doesn&#39;t make average performance in this test any faster. (Although it does halve memory usage and smooth out performance.)

There&#39;s an old saying with performance tuning:

You can&#39;t make the computer faster. You can only make it do less work.

How do we make the computer do less work here? There&#39;s lots of performance wins to be had from going through the code and improving lots of small things. But the automerge team has the right approach. It&#39;s always best to start with macro optimizations. Fix the core algorithm and data structures before moving to optimizing individual methods. There&#39;s no point optimizing a function when you&#39;re about to throw it away in a rewrite.

By far, Automerge&#39;s biggest problem is its complex tree based data structure. And we can replace it with something faster.

Luckily, there&#39;s a better way to implement CRDTs, pioneered in Yjs. Yjs is another (competing) opensource CRDT implementation made by Kevin Jahns. It&#39;s fast, well documented and well made. If I were going to build software which supports collaborative editing today, I&#39;d use Yjs.

Yjs doesn&#39;t need a whole blog post talking about how to make it fast because it&#39;s already pretty fast, as we&#39;ll see soon. It got there by using a clever, obvious data structure &#34;trick&#34; that I don&#39;t think anyone else in the field has noticed. Instead of implementing the CRDT as a tree like automerge does:

Yjs just puts all the items in a single flat list:

That looks simple, but how do you insert a new item into a list? With automerge it&#39;s easy:

But with this list approach it&#39;s more complicated:

Essentially, this approach is just a fancy insertion sort. We&#39;re implementing a list CRDT with a list. Genius!

This sounds complicated - how do you figure out where the new item should go? But it&#39;s complicated in the same way math is complicated. It&#39;s hard to understand, but once you understand it, you can implement the whole insert function in about 20 lines of code:

(But don&#39;t be alarmed if this looks confusing - we could probably fit everyone on the planet who understands this code today into a small meeting room.)

I implemented both Yjs&#39;s CRDT (YATA) and Automerge using this approach in my experimental reference-crdts codebase. Here&#39;s the insert function, with a few more comments. The Yjs version of this function is in the same file, if you want to have a look. Despite being very different papers, the logic for inserting is almost identical. And even though my code is very different, this approach is semantically identical to the actual automerge, and Yjs and sync9 codebases. (Fuzzer verified (TM)).

If you&#39;re interested in going deeper on this, I gave a talk about this approach at a braid meeting a few weeks ago.

The important point is this approach is better:

Theoretically this algorithm can slow down when there are concurrent inserts in the same location in the document. But that&#39;s really rare in practice - you almost always just insert right after the parent item.

Using this approach, my implementation of automerge&#39;s algorithm is about 10x faster than the real automerge. And it&#39;s 30x more memory-efficient:

I wish I could attribute all of that difference to this sweet and simple data structure. But a lot of the difference here is probably just immutablejs gumming automerge up.

It&#39;s a lot faster than automerge:



We&#39;re using a clean and fast core data abstraction now, but the implementation is still not fast. There are two big performance bottlenecks in this codebase we need to fix:

(These lines are marked (1) and (2) in the code listing above).

To understand why this code is necessary, lets say we have a document, which is a list of items.

And some of those items might have been deleted. I&#39;ve added an isDeleted flag to mark which ones. (Unfortunately we can&#39;t just remove them from the array because other inserts might depend on them. Drat! But that&#39;s a problem for another day.)

Imagine the document has 150 000 array items in it, representing 100 000 characters which haven&#39;t been deleted. If the user types an &#39;a&#39; in the middle of the document (at document position 50 000), what index does that correspond to in our array? To find out, we need to scan through the document (skipping deleted items) to figure out the right array location.

So if the user inserts at position 50 000, we&#39;ll probably have to linearly scan past 75 000 items or something to find the insert position. Yikes!

And then when we actually insert, the code does this, which is double yikes:

If the array currently has 150 000 items, javascript will need to move every single item after the new item once space forward in the array. This part happens in native code, but it&#39;s still probably slow when we&#39;re moving so many items. (Aside: V8 is actually suspiciously fast at this part, so maybe v8 isn&#39;t using an array internally to implement Arrays? Who knows!)

But in general, inserting an item into a document with n items will take about n steps. Wait, no - it&#39;s worse than that because deleted items stick around. Inserting into a document where there have ever been n items will take n steps. This algorithm is reasonably fast, but it gets slower with every keystroke. Inserting n characters will take O(n^2).

You can see this if we zoom in on the diagram above. There&#39;s a lot going on here because Martin&#39;s editing position bounced around the document. But there&#39;s a strong linear trend up and to the right, which is what we would expect when inserts take O(n) time:



And why this shape in particular? And why does performance get better near the end? If we simply graph where each edit happened throughout the editing trace, with the same bucketing and smoothing, the result is a very familiar curve:



It looks like the time spent applying changes is dominated by the time it takes to scan through the document&#39;s array.

Can we fix this? Yes we can! And by &#34;we&#34;, I mean Kevin fixed these problems in Yjs. How did he manage that?

So remember, there are two problems to fix:

Kevin solved the first problem by thinking about how humans actually edit text documents. Usually while we&#39;re typing, we don&#39;t actually bounce around a document very much. Rather than scanning the document each time an edit happens, Yjs caches the last (index, position) pair where the user made an edit. The next edit will probably be pretty close to the previous edit, so Kevin just scans forwards or backwards from the last editing position. This sounds a little bit dodgy to me - I mean, thats a big assumption to make! What if edits happen randomly?! But people don&#39;t actually edit documents randomly, so it works great in practice.

(What if two users are editing different parts of a document at the same time? Yjs actually stores a whole set of cached locations, so there&#39;s almost always a cached cursor location near each user no matter where they&#39;re making changes in the document.)

Once Yjs finds the target insert location, it needs to insert efficiently, without copying all the existing items. Yjs solves that by using a bidirectional linked list instead of an array. So long as we have an insert position, linked lists allow inserts in constant time.

Yjs does one more thing to improve performance. Humans usually type in runs of characters. So when we type &#34;hello&#34; in a document, instead of storing:

Yjs just stores:

Finally those pesky paste events will be fast too!

This is the same information, just stored more compactly. Unfortunately we can&#39;t collapse the whole document into a single item or something like that using this trick. The algorithm can only collapse inserts when the IDs and parents line up sequentially - but that happens whenever a user types a run of characters without moving their cursor. And that happens a lot.

In this data set, using spans reduces the number of array entries by 14x. (180k entries down to 12k).

How fast is it now? This blows me away - Yjs is 30x faster than my reference-crdts implementation in this test. And it only uses about 10% as much RAM. It&#39;s 300x faster than automerge!.

Honestly I&#39;m shocked and a little suspicious of how little ram Yjs uses in this test. I&#39;m sure there&#39;s some wizardry in V8 making this possible. It&#39;s extremely impressive.

Kevin says he wrote and rewrote parts of Yjs 12 times in order to make this code run so fast. If there was a programmer version of the speedrunning community, they would adore Kevin. I can&#39;t even put Yjs on the same scale as the other algorithms because it&#39;s so fast:



If we isolate Yjs, you can see it has mostly flat performance. Unlike the other algorithms, it doesn&#39;t get slower over time, as the document grows:



But I have no idea what those spikes are near the end. They&#39;re pretty small in absolute terms, but it&#39;s still weird! Maybe they happen when the user moves their cursor around the document? Or when the user deletes chunks? I have no idea.

This is neat, but the real question is: Can we go even faster? Honestly I doubt I can make pure javascript run this test any faster than Kevin managed here. But maybe.. just maybe we can be...

When I told Kevin that I thought I could make a CRDT implementation that&#39;s way faster than Yjs, he didn&#39;t believe me. He said Yjs was already so well optimized, going a lot faster probably wasn&#39;t possible. &#34;Maybe a little faster if you just port it to Rust. But not a lot faster! V8 is really fast these days!!&#34;

But I knew something Kevin didn&#39;t know: I knew about memory fragmentation and caches. Rust isn&#39;t just faster. It&#39;s also a lower level language, and that gives us the tools we need to control allocations and memory layout.

Kevin knows this now too, and he&#39;s working on Yrs to see if he can claim the performance crown back.

Imagine one of our document items in javascript:

This object is actually a mess like this in memory:



Bad news: Your computer hates this.

This is terrible because all the data is fragmented. It&#39;s all separated by pointers.

And yes, I know, V8 tries its hardest to prevent this sort of thing when it can. But its not magic.

To arrange data like this, the computer has to allocate memory one by one for each item. This is slow. Then the garbage collector needs extra data to track all of those objects, which is also slow. Later we&#39;ll need to read that data. To read it, your computer will often need to go fetch it from main memory, which - you guessed it - is slow as well.

How slow are main memory reads? At human scale each L1 cache read takes 0.5 seconds. And a read from main memory takes close to 2 minutes! This is the difference between a single heartbeat, and the time it takes to brush your teeth.

Arranging memory like javascript does would be like writing a shopping list. But instead of &#34;Cheese, Milk, Bread&#34;, your list is actually a scavenger hunt: &#34;Under the couch&#34;, &#34;On top of the fridge&#34;, and so on. Under the couch is a little note mentioning you need toothpaste. Needless to say, this makes doing the grocery shopping a lot of work.

To go faster, we need to squish all the data together so the computer can fetch more information with each read of main memory. (We want a single read of my grocery list to tell us everything we need to know). Linked lists are rarely used in the real world for exactly this reason - memory fragmentation ruins performance. I also want to move away from linked lists because the user does sometimes hop around the document, which in Yjs has a linear performance cost. Thats probably not a big deal in text editing, but I want this code to be fast in other use cases too. I don&#39;t want the program to ever need those slow scans.

We can&#39;t fix this in javascript. The problem with fancy data structures in javascript is that you end up needing a lot of exotic objects (like fixed size arrays). All those extra objects make fragmentation worse, so as a result of all your work, your programs often end up running slower anyway. This is the same limitation immutablejs has, and why its performance hasn&#39;t improved much in the decade since it was released. The V8 optimizer is very clever, but it&#39;s not magic and clever tricks only get us so far.

But we&#39;re not limited to javascript. Even when making webpages, we have WebAssembly these days. We can code this up in anything.

To see how fast we can really go, I&#39;ve been quietly building a CRDT implementation in rust called Diamond types. Diamond is almost identical to Yjs, but it uses a range tree instead of a linked list internally to store all of the items.

Under the hood, my range tree is just a slightly modified b-tree. But usually when people talk about b-trees they mean a BTreeMap. Thats not what I&#39;m doing here. Instead of storing keys, each internal node of the b-tree stores the total number of characters (recursively) in that item&#39;s children. So we can look up any item in the document by character position, or insert or delete anywhere in the document in log(n) time.

This example shows the tree storing a document which currently has 1000 characters:



This is a range tree, right? The wikipedia article on range trees is a pretty weak description of what I&#39;m doing here.

This solves both of our linear scanning problems from earlier:

We never merge edits from remote peers in this test, but I made that fast too anyway. When merging remote edits we also need to find items by their ID (eg [&#39;seph&#39;, 100]). Diamond has little index to search the b-tree by ID. That codepath doesn&#39;t get benchmarked here though. It&#39;s fast but for now you&#39;ll have to take my word for it.

I&#39;m not using Yjs&#39;s trick of caching the last edit location - at least not yet. It might help. I just haven&#39;t tried it yet.

Rust gives us total control over the memory layout, so we can pack everything in tightly. Unlike in the diagram, each leaf node in my b-tree stores a block of 32 entries, packed in a fixed size array in memory. Inserting with a structure like this results in a little bit of memcpy-ing, but a little bit of memcpy is fine. Memcpy is always faster than I think it will be - CPUs can copy several bytes per clock cycle. Its not the epic hunt of a main memory lookup.

And why 32 entries? I ran this benchmark with a bunch of different bucket sizes and 32 worked well. I have no idea why that worked out to be the best.

Speaking of fast, how fast does it go?

If we compile this code to webassembly and drive it from javascript like in the other tests, we can now process the whole editing trace in 193 milliseconds. Thats 5x faster than Yjs. And remarkably 3x faster than our baseline test editing a native javascript string, despite doing all the work to support collaborative editing!

Javascript and WASM is now a bottleneck. If we skip javascript and run the benchmark directly in rust, we can process all 260k edits in this editing trace in just 56 milliseconds. That&#39;s over 5000x faster than where we started with automerge. It can process 4.6 million operations every second.

Performance is smooth as butter. A b-tree doesn&#39;t care where edits happen. This system is uniformly fast across the whole document. Rust doesn&#39;t need a garbage collector to track memory allocations, so there&#39;s no mysterious GC spikes. And because memory is so tightly packed, processing this entire data set (all 260 000) only results in 1394 calls to malloc.



Oh, what a pity. Its so fast you can barely see it next to yjs (fleexxxx). Lets zoom in a bit there and bask in that flat line:



Well, a nearly flat line.

And remember, this chart shows the slow version. This chart is generated from javascript, calling into rust through WASM. If I run this benchmark natively its another ~4x faster again.

Why is WASM 4x slower than native execution? Are javascript calls to the WASM VM really that slow? Does LLVM optimize native x86 code better? Or do WASM&#39;s memory bounds checks slow it down? I&#39;m so curious!

This implementation has another small, important change - and I&#39;m not sure if I like it.

In rust I&#39;m actually doing something like this:

Notice the document&#39;s text content doesn&#39;t live in the list of items anymore. Now it&#39;s in a separate data structure. I&#39;m using a rust library for this called Ropey. Ropey implements another b-tree to efficiently manage just the document&#39;s text content.

This isn&#39;t universally a win. We have unfortunately arrived at the Land of Uncomfortable Engineering Tradeoffs:

So I&#39;m still not sure whether I like this approach.

But regardless, my CRDT implementation is so fast at this point that most of the algorithm&#39;s time is spent updating the document contents in ropey. Ropey on its own takes 29ms to process this editing trace. What happens if I just ... turn ropey off? How fast can this puppy can really go?

Boom. This is kind of useless, but it&#39;s now 14000x faster than automerge. We&#39;re processing 260 000 operations in 23ms. Thats 11 million operations per second. I could saturate my home internet connection with keystrokes and I&#39;d still have CPU to spare.

We can calculate the average speed each algorithm processes edits:



But these numbers are misleading. Remember, automerge and ref-crdts aren&#39;t steady. They&#39;re fast at first, then slow down as the document grows. Even though automerge can process about 900 edits per second on average (which is fast enough that users won&#39;t notice), the slowest edit during this benchmark run stalled V8 for a full 1.8 seconds.

We can put everything in a single, pretty chart if I use a log scale. It&#39;s remarkable how tidy this looks:



Huh - look at the bottom two lines. The jitteryness of yjs and diamond mirror each other. Periods when yjs gets slower, diamond gets faster. I wonder whats going on there!

But log scales are junk food for your intuition. On a linear scale the data looks like this:



That, my friends, is how you make the computer do a lot less work.

That silly academic paper I read all those years ago says some CRDTs and OT algorithms are slow. And everyone believed the paper, because it was Published Science. But the paper was wrong. As I&#39;ve shown, we can make CRDTs fast. We can make them crazy fast if we get creative with our implementation strategies. With the right approach, we can make CRDTs so fast that we can compete with the performance of native strings. The performance numbers in that paper weren&#39;t just wrong. They were &#34;a billionaire guessing a banana costs $1000&#34; kind of wrong.

But you know what? I sort of appreciate that paper now. Their mistake is ok. It&#39;s human. I used to feel inadequate around academics - maybe I&#39;ll never be that smart! But this whole thing made me realise something obvious: Scientists aren&#39;t gods, sent from the heavens with the gift of Truth. No, they&#39;re beautiful, flawed people just like the rest of us mooks. Great at whatever we obsess over, but kind of middling everywhere else. I can optimize code pretty well, but I still get zucchini and cucumber mixed up. And, no matter the teasing I get from my friends, thats ok.

A decade ago Google Wave really needed a good quality list CRDT. I got super excited when the papers for CRDTs started to emerge. LOGOOT and WOOT seemed like a big deal! But that excitement died when I realised the algorithms were too slow and inefficient to be practically useful. And I made a big mistake - I assumed if the academics couldn&#39;t make them fast, nobody could.

But sometimes the best work comes out of a collaboration between people with different skills. I&#39;m terrible at academic papers, I&#39;m pretty good at making code run fast. And yet here, in my own field, I didn&#39;t even try to help. The researchers were doing their part to make P2P collaborative editing work. And I just thumbed my nose at them all and kept working on Operational Transform. If I helped out, maybe we would have had fast, workable CRDTs for text editing a decade ago. Oops! It turned out collaborative editing needed a collaboration between all of us. How ironic! Who could have guessed?!

Well, it took a decade, some hard work and some great ideas from a bunch of clever folks. The binary encoding system Martin invented for Automerge is brilliant. The system of avoiding UUIDs by using incrementing (agent id, sequence) tuples is genius. I have no idea who came up with that, but I love it. And of course, Kevin&#39;s list representation + insertion approach I describe here makes everything so much faster and simpler. I bet 100 smart people must have walked right past that idea over the last decade without any of them noticing it. I doubt I would have thought of it either. My contribution is using run-length encoded b-trees and clever indexing. And showing Kevin&#39;s fast list representation can be adapted to any CRDT algorithm. I don&#39;t think anyone noticed that before.

And now, after a decade of waiting, we finally figured out how to make fast, lightweight list CRDT implementations. Practical decentralized realtime collaborative editing? We&#39;re coming for you next.

If you&#39;re building a document based collaborative application today, you should use Yjs. Yjs has solid performance, low memory usage and great support. If you want help implementing Yjs in your application, Kevin Jahns sometimes accepts money in exchange for help integrating Yjs into various applications. He uses this to fund working on Yjs (and adjacent work) full time. Yjs already runs fast and soon it should become even faster.

The automerge team is also fantastic. I&#39;ve had some great conversations with them about these issues. They&#39;re making performance the #1 issue of 2021 and they&#39;re planning on using a lot of these tricks to make automerge fast. It might already be much faster by the time you&#39;re reading this.

Diamond is really fast, but there&#39;s a lot of work before I have feature parity with Yjs and Automerge. There is a lot more that goes into a good CRDT library than operation speed. CRDT libraries also need to support binary encoding, network protocols, non-list data structures, presence (cursor positions), editor bindings and so on. At the time of writing, diamond does almost none of this.

If you want database semantics instead of document semantics, as far as I know nobody has done this well on top of CRDTs yet. You can use ShareDB, which uses OT. I wrote ShareDB years ago, and it&#39;s well used, well maintained and battle tested.

Looking forward, I&#39;m excited for Redwood - which supports P2P editing and has planned full CRDT support.

Is this for real? Yes. But performance is complicated and I&#39;m not telling the full picture here.

First, if you want to play with any of the benchmarks I ran yourself, you can. But everything is a bit of a mess.

The benchmark code for the JS plain string editing baseline, Yjs, automerge and reference-crdts tests is all in this github gist. It&#39;s a mess; but messy code is better than missing code.

You&#39;ll also need automerge-paper.json.gz from josephg/crdt-benchmarks in order to run most of these tests. The reference-crdts benchmark depends on crdts.ts from josephg/reference-crdts, at this version.

Diamond&#39;s benchmarks come from josephg/diamond-types, at this version. Benchmark by running  RUSTFLAGS=&#39;-C target-cpu=native&#39; cargo criterion yjs. The inline rope structure updates can be enabled or disabled by editing the constant at the top of src/list/doc.rs. You can look at memory statistics by running cargo run --release --features memusage --example stats.

Diamond is compiled to wasm using this wrapper, hardcoded to point to a local copy of diamond-types from git. The wasm bundle is optimized with wasm-opt.

The charts were made on ObservableHQ.

Throughout this post I&#39;ve been comparing the performance of implementations of RGA (automerge) and YATA (Yjs + my rust implementation) interchangeably.

Doing this rests on the assumption that the concurrent merging behaviour for YATA and RGA are basically the same, and that you can swap between CRDT behaviour without changing your implementation, or your implementation performance. This is a novel idea that I think nobody has looked at before.

I feel confident in this claim because I demonstrated it in my reference CRDT implementation, which has identical performance (and an almost identical codepath) when using Yjs or automerge&#39;s behaviour. There might be some performance differences with conflict-heavy editing traces - but that&#39;s extremely rare in practice.

I&#39;m also confident you could modify Yjs to implement RGA&#39;s behaviour if you wanted to, without changing Yjs&#39;s performance. You would just need to:

I talked to Kevin about this, and he doesn&#39;t see any point in adding RGA support into his library. It&#39;s not something anybody actually asks for. And RGA can have weird interleaving when prepending items.

For diamond, I make my code accept a type parameter for switching between Yjs and automerge&#39;s behaviour. I&#39;m not sure if I want to. Kevin is probably right - I don&#39;t think this is something people ask for.

Well, there is one way in which Yjs has a definite edge over automerge: Yjs doesn&#39;t record when each item in a document has been deleted. Only whether each item has been deleted or not. This has some weird implications:

For now, the master branch of diamond includes temporal deletes. But all benchmarks in this blog post use a yjs-style branch of diamond-types, which matches how Yjs works instead. This makes for a fairer comparison with yjs, but diamond 1.0 might have a slightly different performance profile. (There&#39;s plenty of puns here about diamond not being polished yet, but I&#39;m not sharp enough for those right now.)

This post only measures the time taken to replay a local editing trace. And I&#39;m measuring the resulting RAM usage. Arguably accepting incoming changes from the user only needs to happen fast enough. Fingers simply don&#39;t type very fast. Once a CRDT can handle any local user edit in under about 1ms, going faster probably doesn&#39;t matter much. (And automerge usually performs that well already, barring some unlucky GC pauses.)

The actually important metrics are:

The editing trace I&#39;m using here also only has a single user making edits. There could be pathological performance cases lurking in the shadows when users make concurrent edits.

I did it this way because I haven&#39;t implemented a binary format in my reference-crdts implementation or diamond yet. If I did, I&#39;d probably copy Yjs &amp; automerge&#39;s binary formats because they&#39;re so compact. So I expect the resulting binary size would be similar between all of these implementations, except for delete operations. Performance for loading and saving will probably approximately mirror the benchmarks I showed above. Maybe. Or maybe I&#39;m wrong. I&#39;ve been wrong before. It would be fun to find out.

There&#39;s one other performance measure I think nobody is taking seriously enough at the moment. And that is, how we update a document at rest (in a database). Most applications aren&#39;t collaborative text editors. Usually applications are actually interacting with databases full of tiny objects. Each of those objects is very rarely written to.

If you want to update a single object in a database using Yjs or automerge today you need to:

This is going to be awfully slow. There are better approaches for this - but as far as I know, nobody is working on this at all. We could use your help!

Edit: Kevin says you can adapt Yjs&#39;s providers to implement this in a reasonable way. I&#39;d love to see that in action.

There&#39;s another approach to making CRDTs fast, which I haven&#39;t mentioned here at all and that is pruning. By default, list CRDTs like these only ever grow over time (since we have to keep tombstones for all deleted items). A lot of the performance and memory cost of CRDTs comes from loading, storing and searching that growing data set. There are some approaches which solve this problem by finding ways to shed some of this data entirely. For example, Yjs&#39;s GC algorithm, or Antimatter. That said, git repositories only ever grow over time and nobody seems mind too much. Maybe it doesn&#39;t matter so long as the underlying system is fast enough?

But pruning is orthogonal to everything I&#39;ve listed above. Any good pruning system should also work with all of the algorithms I&#39;ve talked about here.

Each step in this optimization journey involves changes to multiple variables and I&#39;m not isolating those changes. For example, moving from automerge to my reference-crdts implementation changed:

We got 10x performance from all this. But I&#39;m only guessing how that 10x speedup should be distributed amongst all those changes.

The jump from reference-crdts to Yjs, and from Yjs to diamond are similarly monolithic. How much of the speed difference between diamond and Yjs has nothing to do with memory layout, and everything to do with LLVM&#39;s optimizer?

The fact that automerge-rs isn&#39;t faster than automerge gives me some confidence that diamond&#39;s performance isn&#39;t just thanks to rust. But I honestly don&#39;t know.

So, yes. This is a reasonable criticism of my approach. If this problem bothers you, I&#39;d love for someone to pull apart each of the performance differences between implementations I show here and tease apart a more detailed breakdown. I&#39;d read the heck out of that. I love benchmarking stories. That&#39;s normal, right?

Because it&#39;s not trying to be fast. Look at this code from automerge:

This is called on each insert, to figure out how the children of an item should be sorted. I don&#39;t know how hot it is, but there are so many things slow about this:

But in practice this code is going to be replaced by WASM calls through to automerge-rs. Maybe it already has been replaced with automerge-rs by the time you&#39;re reading this! So it doesn&#39;t matter. Try not to think about it. Definitely don&#39;t submit any PRs to fix all the low hanging fruit. twitch.

This post is part of the Braid project and funded by the Invisible College. If this is the sort of work you want to contribute towards, get in touch. We&#39;re hiring.

Thankyou to everyone who gave feedback before this post went live.

And special thanks to Martin Kleppmann and Kevin Jahns for their work on Automerge and Yjs. Diamond stands on the shoulders of giants.

Comments on Hacker News

2021 Seph Gentle

https://github.com/josephg/
]


---
### 标题: A Collection of Free Public APIs That Is Tested Daily

### 链接: https://www.freepublicapis.com/

#### 正文: [


Search

Favorites

General

Metrics

Do Something

Categories

FHGR

Search

Favorites

General

Metrics

Do Something

Categories

FHGR

Search

APIs with the highest Health Scores. Health is calculated from latency, error rate and reliability.

Free IP Geolocation API - lookup any IP address. Provides geolocation data based on the input IPv4/IPv6 address or domain name, with optional parameters for customization like language and callback.

1 Endpoint

100 API Health

Free fake and reliable API for testing and prototyping. Serving ~3 billion requests each month.

3 Endpoints

100 API Health

API providing data on Harry Potter characters and spells. Includes routes for all characters, specific character by ID, Hogwarts students and staff, characters in a house, and all spells.

3 Endpoints

100 API Health

Data Dragon is a static data API providing information and images related to champions, runes, and items in League of Legends. It allows developers to access and download data files, enabling them to integrate game-related information into their applications.

1 Endpoint

100 API Health

The JSON IP Geolocation API allows you to determine the geographical location of visitors based on their IP addresses. It provides detailed location data such as country, region, city, and coordinates in a structured JSON format.

2 Endpoints

100 API Health

An API for public holiday data. Supports over 100 countries. Provides information on holidays and long weekends.

1 Endpoint

100 API Health

API providing information about characters, locations, and episodes from the Rick and Morty show. Supports GraphQL and REST endpoints for querying data.

3 Endpoints

100 API Health

A free REST API for random Kanye West quotes (Kanye as a Service)

1 Endpoint

100 API Health

Open-Meteo is an open-source weather API and offers free access for non-commercial use. No API key required. Start using it now!

2 Endpoints

100 API Health

An open RESTful API for Pokémon data

1 Endpoint

100 API Health

HTTP API for useless facts. Provides random useless facts and today&#39;s useless fact with language and content-type customization options. APIv1 is deprecated.

3 Endpoints

100 API Health

This API generates a random &#39;Yes&#39; or &#39;No&#39; response.

1 Endpoint

100 API Health

Free UK postcode lookup API and datasets. Search, validate and reverse geocode postcodes. Open sourced project.

2 Endpoints

100 API Health

Pollutants and pollen forecast in 11 km resolution

1 Endpoint

100 API Health

Simulated river discharge at 5 km resolution from 1984 up to 7 months forecast.

1 Endpoint

100 API Health

nekos.best is a RESTful API serving fully SFW and high quality anime images and GIFs.

3 Endpoints

100 API Health

Browse 47,000+ automated reports, view aggregated data, merge and download data, compare reports

1 Endpoint

100 API Health
]


---
### 标题: Tesla&#39;s TTPoE at Hot Chips 2024: Replacing TCP for Low Latency Applications

### 链接: https://chipsandcheese.com/2024/08/27/teslas-ttpoe-at-hot-chips-2024-replacing-tcp-for-low-latency-applications/

#### 正文: [




Tesla’s TTPoE at Hot Chips 2024: Replacing TCP for Low Latency Applications




August 27, 2024
clamchowder

Leave a comment



Last year at Hot Chips 2023, Tesla introduced their Dojo supercomputer. For Tesla, machine learning is focused on automotive applications like self driving cars. Training deals with video, which can demand a lot of IO bandwidth. As an example, the size of a single tensor could be 1.7 GB for the company’s vision applications. Tesla found that throughput for their Dojo supercomputer could be limited by how fast host machines can push data to the supercomputer, even if the hosts were doing nothing but copying data through PCIe.


Tesla tackles this problem by adding more hosts, and a cheap way to connect those extra hosts to the supercomputer. Instead of using typical supercomputer networking solutions like Infiniband, Tesla chose to adapt Ethernet to their needs with a modified transport layer. TCP gets replaced by Tesla Transport Protocol over Ethernet, or TTPoE. TTPoE is designed both to offer microsecond scale latency and allow simple hardware offload. Lower level layers remain unchanged, letting the protocol run over standard Ethernet switches.


TTPoE is designed to be handled completely in hardware and deliver better latency than the standard TCP protocol. TTPoE’s state machine is therefore significantly simplified compared to the TCP one.


Latency is reduced by removing wait states in TCP. Closing a connection in TCP involves sending a FIN, waiting for an acknowledgement of that FIN, and acknowledging the acknowledgement in return. After that, the connection enters a TIME WAIT state that requires the implementation to wait for some time, allowing any out-of-order packets to safely drain before a new connection can reuse the port. TTP deletes the TIME_WAIT state, and changes the closing sequence from three transmissions to two. A TTP connection can be closed by sending a close opcode, and receiving an acknowledgement of that. Tesla is targeting latencies on the microsecond scale, so even a TIME_WAIT duration on a millisecond level could cause significant problems.


TCP famously opens connections with a three way SYN, SYN-ACK, ACK handshake. TTP applies a similar optimization as on the closing end, changing the handshake to a two-way one. Again, opening a connection with fewer transmissions cuts down on latency. Those simplified open and close sequences are implemented in hardware, which also makes it transparent to software. I take this to mean software doesn’t have to explicitly create connections, and rather can tell the hardware which destination it wants to send data to or receive data from. 


Like TCP, Tesla uses packet drops for congestion control. But because TTP is designed to run over a low latency underlying network, Tesla was able to take a brute force approach to the problem. A traditional TCP implementation maintains a sliding congestion window that limits how much un-acknowledged data can be sent. You can look at this as much traffic is in-flight in the network. That congestion window scales up if packets are acknowledged promptly, increasing bandwidth. It rapidly scales down if packets are dropped and acknowledgements are not received within a time threshold. That lets TCP gracefully handle a variety of different connections. Bandwidth will scale up in a low latency, low loss home local network, and naturally scale down over a high latency, high packet loss link to your internet service provider and beyond. 


Tesla doesn’t intend to run TTP over the low quality links of the open internet, and therefore takes a brute force approach to congestion control. The congestion window is not scaled depending on packet loss. Hardware tracks sent data in a SRAM buffer, which defines the congestion window size. Sending stops when the buffer fills, and packet drops are handled by retransmitting data held in the SRAM buffer. Data is deallocated from the SRAM buffer when its corresponding acknowledgement comes back from the other side, naturally moving the sliding window forward.

TCP uses a variable congestion window size based on network conditions, which TPP doesn’t do
Tesla justified this approach by noting that traditional TCP congestion control algorithms like Reno work on too long of a timescale to be effective for their Dojo supercomputer application.


Congestion management is independently handled at each endpoint, in a model familiar to TCP enjoyers. Tesla mentioned this primarily to draw a contrast to other low latency networks like Infiniband, where congestion control is handled at the switch level. Infiniband uses a credit system controlled at the switch level, and does not drop packets. If an endpoint runs out of credits, it simply stops sending. TCP and TTP’s approach of handling congestion by simply dropping packets eliminates the need for separately sending credits and reduces complexity at network switches.


Tesla handles their TTP protocol in a hardware block placed between a chip and standard Ethernet hardware. This MAC hardware block was designed by a CPU architect, and brings in a lot of CPU design features. The presenter described it as acting like a shared cache, where an arbiter picks between requests with ordering hazards in mind.


In-flight packets are “retired” in-order when they’ve been ack-ed, in a scheme reminiscent of a CPU retiring instructions in-order from a reorder buffer. One of the most prominent resources is a 1 MB transmit SRAM buffer, which defines the congestion window as mentioned above. Tesla says this size is adequate to tolerate about 80 microseconds of network latency without significant bandwidth loss. Doing the math with Little’s Law, given 1 MB of in-flight data and 80 microseconds of latency, would give 97.65Gbps. That’s just about enough to saturate a 100 gigabit network interface.
The TPP MAC is implemented on what Tesla calls a “Dumb-NIC”. NIC stands for “Network Interface Card”. It’s called “dumb” because it’s as cheap and simple as possible. Tesla wants to deploy large numbers of host nodes to feed their Dojo supercomputer, and having cheap network cards helps achieve that in a cost efficient manner.


Besides the TPP MAC, Mojo incorporates a host chip with a PCIe Gen 3 x16 interface, along with 8 GB of DDR4. PCIe Gen 3 and DDR4 are not cutting edge technologies, but help keep cost under control. The Mojo name comes from the idea that extra host nodes give Dojo more Mojo to keep performance up.

Presenter showing a Mojo NIC. Note the heatsink to deal with the 20W power draw
These Mojo cards are installed into remote host machines. When engineers need more bandwidth to feed data into the Dojo supercomputer, remote host machines can be pulled from the pool. Additional bandwidth from those machines stacks on top of ingress bandwidth provided by existing host machines using the higher cost Interface Processor presented at last year’s Hot Chips conference.


Overall, Mojo and the TTPoE protocol provide an interesting look at how the well know Transmission Control Protocol (TCP) can be simplified for use with a higher quality intra-supercomputer network. While the protocol could theoretically run over the internet, simplifications like a fixed congestion window wouldn’t work well over the lower quality links to internet service providers and beyond. 


Compared to other supercomputing network solutions like Infiniband, a custom transport protocol over Ethernet might provide enough extra bandwidth to meet Dojo’s needs. We’d like to thank Tesla for giving a presentation, and showing off hardware on-stage.
If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our Patreon or our PayPal if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our Discord.

Author







 


clamchowder 



View all posts


 

















Please leave this field empty Don’t miss our articles!

Email Address *






Check your inbox or spam folder to confirm your subscription.

 




Related Posts








Last year at Hot Chips 2023, Tesla introduced their Dojo supercomputer. For Tesla, machine learning is focused on automotive applications like self driving cars. Training deals with video, which can demand a lot of IO bandwidth. As an example, the size of a single tensor could be 1.7 GB for the company’s vision applications. Tesla found that throughput for their Dojo supercomputer could be limited by how fast host machines can push data to the supercomputer, even if the hosts were doing nothing but copying data through PCIe.

Tesla tackles this problem by adding more hosts, and a cheap way to connect those extra hosts to the supercomputer. Instead of using typical supercomputer networking solutions like Infiniband, Tesla chose to adapt Ethernet to their needs with a modified transport layer. TCP gets replaced by Tesla Transport Protocol over Ethernet, or TTPoE. TTPoE is designed both to offer microsecond scale latency and allow simple hardware offload. Lower level layers remain unchanged, letting the protocol run over standard Ethernet switches.

TTPoE is designed to be handled completely in hardware and deliver better latency than the standard TCP protocol. TTPoE’s state machine is therefore significantly simplified compared to the TCP one.

Latency is reduced by removing wait states in TCP. Closing a connection in TCP involves sending a FIN, waiting for an acknowledgement of that FIN, and acknowledging the acknowledgement in return. After that, the connection enters a TIME WAIT state that requires the implementation to wait for some time, allowing any out-of-order packets to safely drain before a new connection can reuse the port. TTP deletes the TIME_WAIT state, and changes the closing sequence from three transmissions to two. A TTP connection can be closed by sending a close opcode, and receiving an acknowledgement of that. Tesla is targeting latencies on the microsecond scale, so even a TIME_WAIT duration on a millisecond level could cause significant problems.

TCP famously opens connections with a three way SYN, SYN-ACK, ACK handshake. TTP applies a similar optimization as on the closing end, changing the handshake to a two-way one. Again, opening a connection with fewer transmissions cuts down on latency. Those simplified open and close sequences are implemented in hardware, which also makes it transparent to software. I take this to mean software doesn’t have to explicitly create connections, and rather can tell the hardware which destination it wants to send data to or receive data from. 

Like TCP, Tesla uses packet drops for congestion control. But because TTP is designed to run over a low latency underlying network, Tesla was able to take a brute force approach to the problem. A traditional TCP implementation maintains a sliding congestion window that limits how much un-acknowledged data can be sent. You can look at this as much traffic is in-flight in the network. That congestion window scales up if packets are acknowledged promptly, increasing bandwidth. It rapidly scales down if packets are dropped and acknowledgements are not received within a time threshold. That lets TCP gracefully handle a variety of different connections. Bandwidth will scale up in a low latency, low loss home local network, and naturally scale down over a high latency, high packet loss link to your internet service provider and beyond. 

Tesla doesn’t intend to run TTP over the low quality links of the open internet, and therefore takes a brute force approach to congestion control. The congestion window is not scaled depending on packet loss. Hardware tracks sent data in a SRAM buffer, which defines the congestion window size. Sending stops when the buffer fills, and packet drops are handled by retransmitting data held in the SRAM buffer. Data is deallocated from the SRAM buffer when its corresponding acknowledgement comes back from the other side, naturally moving the sliding window forward.

Tesla justified this approach by noting that traditional TCP congestion control algorithms like Reno work on too long of a timescale to be effective for their Dojo supercomputer application.

Congestion management is independently handled at each endpoint, in a model familiar to TCP enjoyers. Tesla mentioned this primarily to draw a contrast to other low latency networks like Infiniband, where congestion control is handled at the switch level. Infiniband uses a credit system controlled at the switch level, and does not drop packets. If an endpoint runs out of credits, it simply stops sending. TCP and TTP’s approach of handling congestion by simply dropping packets eliminates the need for separately sending credits and reduces complexity at network switches.

Tesla handles their TTP protocol in a hardware block placed between a chip and standard Ethernet hardware. This MAC hardware block was designed by a CPU architect, and brings in a lot of CPU design features. The presenter described it as acting like a shared cache, where an arbiter picks between requests with ordering hazards in mind.

In-flight packets are “retired” in-order when they’ve been ack-ed, in a scheme reminiscent of a CPU retiring instructions in-order from a reorder buffer. One of the most prominent resources is a 1 MB transmit SRAM buffer, which defines the congestion window as mentioned above. Tesla says this size is adequate to tolerate about 80 microseconds of network latency without significant bandwidth loss. Doing the math with Little’s Law, given 1 MB of in-flight data and 80 microseconds of latency, would give 97.65Gbps. That’s just about enough to saturate a 100 gigabit network interface.

The TPP MAC is implemented on what Tesla calls a “Dumb-NIC”. NIC stands for “Network Interface Card”. It’s called “dumb” because it’s as cheap and simple as possible. Tesla wants to deploy large numbers of host nodes to feed their Dojo supercomputer, and having cheap network cards helps achieve that in a cost efficient manner.

Besides the TPP MAC, Mojo incorporates a host chip with a PCIe Gen 3 x16 interface, along with 8 GB of DDR4. PCIe Gen 3 and DDR4 are not cutting edge technologies, but help keep cost under control. The Mojo name comes from the idea that extra host nodes give Dojo more Mojo to keep performance up.

These Mojo cards are installed into remote host machines. When engineers need more bandwidth to feed data into the Dojo supercomputer, remote host machines can be pulled from the pool. Additional bandwidth from those machines stacks on top of ingress bandwidth provided by existing host machines using the higher cost Interface Processor presented at last year’s Hot Chips conference.

Overall, Mojo and the TTPoE protocol provide an interesting look at how the well know Transmission Control Protocol (TCP) can be simplified for use with a higher quality intra-supercomputer network. While the protocol could theoretically run over the internet, simplifications like a fixed congestion window wouldn’t work well over the lower quality links to internet service providers and beyond. 

Compared to other supercomputing network solutions like Infiniband, a custom transport protocol over Ethernet might provide enough extra bandwidth to meet Dojo’s needs. We’d like to thank Tesla for giving a presentation, and showing off hardware on-stage.

If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our Patreon or our PayPal if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our Discord.




Check your inbox or spam folder to confirm your subscription.


 

This site uses Akismet to reduce spam. Learn how your comment data is processed.

Subscribe now to keep reading and get access to the full archive.



Type your email… 
 









 
Subscribe 


Continue reading
]


---
### 标题: The Monospace Web

### 链接: https://owickstrom.github.io/the-monospace-web/

#### 正文: [


Monospace fonts are dear to many of us. Some find them more readable,
consistent, and beautiful, than their proportional alternatives. Maybe
we’re just brainwashed from spending years in terminals? Or are we
hopelessly nostalgic? I’m not sure. But I like them, and that’s why I
started experimenting with all-monospace Web.

On this page, I use a monospace grid to align text and draw diagrams.
It’s generated from a simple Markdown document (using Pandoc), and the
CSS and a tiny bit of Javascript renders it on the grid. The page is
responsive, shrinking in character-sized steps. Standard elements should
just work, at least that’s the goal. It’s semantic HTML,
rendered as if we were back in the 70s.

All right, but is this even a good idea? It’s a technical and
creative challenge and I like the aestethic. If you’d like to use it,
feel free to fork or copy the bits you need, respecting the license. I
might update it over time with improvements and support for more
standard elements.

This document uses a few extra classes here and there, but mostly
it’s just markup. This, for instance, is a regular paragraph.

Look at this horizontal break:

Lovely. We can hide stuff in the &lt;details&gt;
element:


Hidden gems.


This is a plain old bulleted list:

Ordered lists look pretty much as you’d expect:

It’s nice to visualize trees. This is a regular unordered list with a
tree class:


/dev/nvme0n1p2


We can use regular tables that automatically adjust to the monospace
grid. They’re responsive.

Note that only one column is allowed to grow.

Here are some buttons:

And inputs:

Add the grid class to a container to divide up the
horizontal space evenly for the cells. Not that it maintains the
monospace, so the total width might not be 100%. Here are six grids with
increasing cell count:

If we want one cell to fill the remainder, we set
flex-grow: 1; for that particular cell.

We can draw in &lt;pre&gt; tags using box-drawing
characters:

To have it stand out a bit more, we can wrap it in a
&lt;figure&gt; tag, and why not also add a
&lt;figcaption&gt;.

Let’s go wild and draw a chart!

Media objects are supported, like images and video:

They extend to the width of the page, and add appropriate padding in
the bottom to maintain the monospace grid.

That’s it for now. I’ve very much enjoyed making this, pushing my CSS
chops and having a lot of fun with the design. If you like it or even
decide to use it, please let me
know.

The full source code is here: github.com/owickstrom/the-monospace-web

Finally, a massive shout-out to U.S. Graphics Company for all the
inspiration.
]


